---

# Цель

---

**Построить модель, способную обнаруживать и локализовать определенные объекты на изображениях.**


Мы будем внедрять [Single Shot Multibox Detector (SSD)](https://arxiv.org/abs/1512.02325), решать задачу детекции.

---

# Выполнение

---

В разделах ниже кратко описывается реализация.

### Набор данных

Мы будем использовать набор данных BCCD. BCCD Dataset — это небольшой набор данных для обнаружения клеток крови.

```python
{'RBC', 'WBC', 'Platelets'}
```

Каждое изображение может содержать одино или несколько изображений.

У нас есть три вида клеток крови:

- Эритроцит (красная кровяная клетка)

- WBC (белые кровяные клетки)
  
- Тромбоциты

-  воспринимаемая трудность обнаружения (либо `0`, что означает _не трудно_ , либо `1`, что означает _трудно_ )

* Структура `BCCD_dataset`
  
```
  ├── BCCD
  │   ├── Annotations
  │   │       └── BloodImage_00XYZ.xml (364 items)
  │   ├── ImageSets       # Contain four Main/*.txt which split the dataset
  │   └── JPEGImages
  │       └── BloodImage_00XYZ.jpg (364 items)
  ├── dataset
  │   └── mxnet           
  ├── output
  │   ├── label_map.json        
  │   └── TEST_images.json
  │   └── TEST_objects.json
  │   └── TRAIN_images.json
  │   └── TRAIN_objects.json
  ├── create_data_lists.py        
  ├── datasets.py
  ├── detect.py
  ├── eval.py
  ├── export.py
  ├── model.py
  ├── README.md
  ├── test.csv
  ├── train.py
  ├── utils.py
  ```

#### Скачать

Вам необходимо загрузить следующие наборы данных ВССВ датасет –

- [ВССВ датасет](https://github.com/Shenggan/BCCD_Dataset/tree/master/BCCD) (460MB)

### Входные данные для модели

Нам понадобятся три входа.

#### Изображения

Поскольку мы используем вариант SSD300, изображения должны быть размером в `300, 300` пикселях и в формате RGB.

Мы используем базу VGG-16, предварительно обученную на ImageNet, которая уже доступна в модуле PyTorch `torchvision`. [На этой странице](https://pytorch.org/docs/master/torchvision/models.html) подробно описывается предварительная обработка или преобразование, которые нам нужно будет выполнить для использования этой модели — значения пикселей должны быть в диапазоне [0,1], а затем мы должны нормализовать изображение по среднему значению и стандартному отклонению каналов RGB изображений ImageNet.

```python
mean = [0.485, 0.456, 0.406]
std = [0.229, 0.224, 0.225]
```

Кроме того, PyTorch следует соглашению NCHW, которое означает, что размер каналов (C) должен предшествовать размеру размера.

Следовательно, **изображения, подаваемые в модель, должны представлять собойa `Float` tensor размер `N, 3, 300, 300`**, и должны быть нормализованы по указанному выше среднему значению и стандартному отклонению. `N` - размер пакета.

#### Bounding Boxes

Для каждого изображения нам необходимо указать ограничивающие рамки присутствующих на нем объектов наземной репрезентативности в дробных граничных координатах `(x_min, y_min, x_max, y_max)`.

Поскольку количество объектов на любом изображении может варьироваться, мы не можем использовать тензор фиксированного размера для хранения ограничивающих рамок для всего пакета `N` изображений.

Таким образом, **ограничивающие рамки истинности, передаваемые в модель, должны представлять собой список длины `N`, где каждый элемент списка является `Float` tensor размером `N_o, 4`**, где `N_o` количество объектов, присутствующих на данном конкретном изображении.

#### Labels

Для каждого изображения нам необходимо указать метки присутствующих на нем объектов наземной репрезентативности.

Каждая метка должна быть закодирована как целое число от `1` to `3` представляющее три различных типов объектов. Кроме того, мы добавим _background_ (фоновый класс) class с индексом `0`, который указывает на отсутствие объекта в ограничивающем прямоугольнике. (Но, естественно, эта метка фактически не будет использоваться ни для одного из объектов-истин в наборе данных.)

Опять же, поскольку количество объектов на любом изображении может варьироваться, мы не можем использовать тензор фиксированного размера для хранения меток для всего пакета `N` изображений.

Таким образом, **метки истинности, передаваемые в модель, должны представлять собой список длины `N`, где каждый элемент списка является `Long` tensor размером `N_o`**, где `N_o` количество объектов, присутствующих на данном конкретном изображении.

#### Анализ необработанных данных

See `create_data_lists()` in [`utils.py`](https://github.com/Konstantin-39/BCCD/blob/main/utils.py).

Анализируем загруженные данные и сохраняем следующие файлы –

- Файл JSON для каждого разделения со списком абсолютных путей к файлам изображений

- Файл JSON для каждого разделения со списком словарей, содержащих объекты истинности, т. е. ограничивающие рамки в абсолютных координатах границ, их закодированные метки и предполагаемые трудности обнаружения. Словарь в этом списке будет содержать объекты, присутствующие в изображении в предыдущем файле JSON.

- Файл JSON, содержащий label_map словарь меток-индексов, с помощью которого метки кодируются в предыдущем файле JSON.

#### Набор данных PyTorch

См `PascalVOCDataset` in [`datasets.py`](https://github.com/Konstantin-39/BCCD/blob/main/datasets.py).

Это подкласс PyTorch Dataset, используемый для определения наших обучающих и тестовых наборов данных. Ему нужно __len__определить метод, который возвращает размер набора данных, и __getitem__метод, который возвращает iизображение th, ограничивающие рамки объектов на этом изображении и метки для объектов на этом изображении, используя файлы JSON, которые мы сохранили ранее.

Они требуются только на этапе оценки для вычисления метрики средней средней точности (mAP). У нас также есть возможность полностью отфильтровать сложные объекты из наших данных, чтобы ускорить обучение за счет некоторой потери точности.

#### Data Transforms

См `transform()` in [`utils.py`](https://github.com/Konstantin-39/BCCD/blob/main/utils.py).

Эта функция применяет следующие преобразования к изображениям и объектам на них:

- Случайным образом настройте яркость, контрастность, насыщенность и оттенок , каждый с вероятностью 50% и в случайном порядке.

- С вероятностью 50% выполните операцию уменьшения масштаба изображения. Это помогает научиться обнаруживать мелкие объекты. Уменьшенное изображение должно быть в 1и 4раз больше исходного. Окружающее пространство может быть заполнено средним значением данных ImageNet.

- Произвольно обрезать изображение, т. е. выполнить операцию увеличения . Это помогает научиться обнаруживать большие или частичные объекты. Некоторые объекты могут быть даже полностью вырезаны. Размеры обрезки должны быть между 0.3и 1умноженными на исходные размеры. Соотношение сторон должно быть между 0.5и 2. Каждое кадрирование выполняется таким образом, чтобы оставалась по крайней мере одна ограничивающая рамка, которая имеет перекрытие Жаккара либо 0, 0.1, 0.3, 0.5, 0.7, или 0.9, выбранное случайным образом, с обрезанным изображением. Кроме того, любые оставшиеся ограничивающие рамки, центры которых больше не находятся на изображении в результате кадрирования, отбрасываются. Также есть вероятность, что изображение вообще не будет обрезано.

- С вероятностью 50% переверните изображение по горизонтали.

- Изменить размер изображения до 300, 300пикселей. Это требование SSD300.

- Преобразовать все поля из абсолютных в дробные граничные координаты. На всех этапах нашей модели все граничные и центральные координаты будут в дробных формах.

- Нормализуйте изображение с использованием среднего значения и стандартного отклонения данных ImageNet, которые использовались для предварительной тренировки нашей базы VGG.

# Обучение

Прежде чем начать, обязательно сохраните необходимые файлы данных для обучения и оценки. Для этого запустите содержимое [`create_data_lists.py`](https://github.com/Konstantin-39/BCCD/blob/main/create_data_lists.py) указав расположение папок.

См [`train.py`](https://github.com/Konstantin-39/BCCD/blob/main/train.py).

Параметры модели (и ее обучения) находятся в начале файла, поэтому вы можете легко проверить или изменить их при необходимости.

Чтобы обучить модель с нуля , запустите этот файл –

`python train.py`

Чтобы возобновить обучение с контрольной точки , укажите соответствующий файл с checkpointпараметром в начале кода.

# Оценка

См [`eval.py`](https://github.com/Konstantin-39/BCCD/blob/main/eval.py).

Параметры загрузки данных и контрольных точек для оценки модели находятся в начале файла, поэтому вы можете легко проверить или изменить их при необходимости.

Чтобы начать оценку, просто запустите evaluate()функцию с загрузчиком данных и контрольной точкой модели. Необработанные прогнозы для каждого изображения в тестовом наборе получаются и анализируются методом контрольной точки detect_objects(), который реализует этот процесс.


# Вывод

См [`detect.py`](https://github.com/Konstantin-39/BCCD/blob/main/detect.py).

PУкажите модель, которую вы хотите использовать для вывода, с помощью checkpointпараметра в начале кода.

Затем вы можете использовать эту detect()функцию для идентификации и визуализации объектов на RGB-изображении.
